{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms.functional import crop\n",
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r\"C:\\Users\\grk\\git\\StyleForge\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"deepfashion_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "          id                        image_path  pair_id  style  x_1  y_1  x_2  \\\n0          1  data\\validation\\image\\000001.jpg        1      1  199  190  287   \n1          1  data\\validation\\image\\000001.jpg        1      2  204  189  293   \n2          2  data\\validation\\image\\000002.jpg        1      1  170  121  280   \n3          2  data\\validation\\image\\000002.jpg        1      2  176  120  292   \n4          3  data\\validation\\image\\000003.jpg        3      1  151  241  279   \n...      ...                               ...      ...    ...  ...  ...  ...   \n36245  32149  data\\validation\\image\\032149.jpg     2372      1  345  606  707   \n36246  32150  data\\validation\\image\\032150.jpg     2372      1  418  676  636   \n36247  32151  data\\validation\\image\\032151.jpg     2372      1  358  645  801   \n36248  32152  data\\validation\\image\\032152.jpg     2372      1  196  469  674   \n36249  32153  data\\validation\\image\\032153.jpg     2372      1   57  139  376   \n\n        y_2  label  \n0       269      0  \n1       414      1  \n2       215      0  \n3       383      1  \n4       435      2  \n...     ...    ...  \n36245  1290   4940  \n36246  1221   4940  \n36247  1247   4940  \n36248  1221   4940  \n36249   631   4940  \n\n[36250 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>image_path</th>\n      <th>pair_id</th>\n      <th>style</th>\n      <th>x_1</th>\n      <th>y_1</th>\n      <th>x_2</th>\n      <th>y_2</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>data\\validation\\image\\000001.jpg</td>\n      <td>1</td>\n      <td>1</td>\n      <td>199</td>\n      <td>190</td>\n      <td>287</td>\n      <td>269</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>data\\validation\\image\\000001.jpg</td>\n      <td>1</td>\n      <td>2</td>\n      <td>204</td>\n      <td>189</td>\n      <td>293</td>\n      <td>414</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>data\\validation\\image\\000002.jpg</td>\n      <td>1</td>\n      <td>1</td>\n      <td>170</td>\n      <td>121</td>\n      <td>280</td>\n      <td>215</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>data\\validation\\image\\000002.jpg</td>\n      <td>1</td>\n      <td>2</td>\n      <td>176</td>\n      <td>120</td>\n      <td>292</td>\n      <td>383</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>data\\validation\\image\\000003.jpg</td>\n      <td>3</td>\n      <td>1</td>\n      <td>151</td>\n      <td>241</td>\n      <td>279</td>\n      <td>435</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>36245</th>\n      <td>32149</td>\n      <td>data\\validation\\image\\032149.jpg</td>\n      <td>2372</td>\n      <td>1</td>\n      <td>345</td>\n      <td>606</td>\n      <td>707</td>\n      <td>1290</td>\n      <td>4940</td>\n    </tr>\n    <tr>\n      <th>36246</th>\n      <td>32150</td>\n      <td>data\\validation\\image\\032150.jpg</td>\n      <td>2372</td>\n      <td>1</td>\n      <td>418</td>\n      <td>676</td>\n      <td>636</td>\n      <td>1221</td>\n      <td>4940</td>\n    </tr>\n    <tr>\n      <th>36247</th>\n      <td>32151</td>\n      <td>data\\validation\\image\\032151.jpg</td>\n      <td>2372</td>\n      <td>1</td>\n      <td>358</td>\n      <td>645</td>\n      <td>801</td>\n      <td>1247</td>\n      <td>4940</td>\n    </tr>\n    <tr>\n      <th>36248</th>\n      <td>32152</td>\n      <td>data\\validation\\image\\032152.jpg</td>\n      <td>2372</td>\n      <td>1</td>\n      <td>196</td>\n      <td>469</td>\n      <td>674</td>\n      <td>1221</td>\n      <td>4940</td>\n    </tr>\n    <tr>\n      <th>36249</th>\n      <td>32153</td>\n      <td>data\\validation\\image\\032153.jpg</td>\n      <td>2372</td>\n      <td>1</td>\n      <td>57</td>\n      <td>139</td>\n      <td>376</td>\n      <td>631</td>\n      <td>4940</td>\n    </tr>\n  </tbody>\n</table>\n<p>36250 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'data\\\\validation\\\\image\\\\000001.jpg'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[[0]]['image_path'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFashionDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode, transform=None, target_transform=None):\n",
    "        self.df = pd.read_csv(\"deepfashion_train.csv\")\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[[idx]]\n",
    "        image = Image.open(row['image_path'].values[0])\n",
    "        bbox = row['x_1'].values[0], row['x_2'].values[0], row['y_1'].values[0], row['y_2'].values[0]\n",
    "        cropped_image = image.crop((bbox[0], bbox[2], bbox[1], bbox[3]))\n",
    "        label = row['label'].values[0]\n",
    "        if self.transform:\n",
    "            cropped_image = self.transform(cropped_image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return cropped_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DeepFashionDataset(DATA_DIR, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<PIL.Image.Image image mode=RGB size=355x379>, 49)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[123]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "\n",
    "First we convert a bunch of json's to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def convert_dataset(ann_dir):\n",
    "    columns = [\"id\", \"image_path\", \"pair_id\", \"style\", \"x_1\", \"y_1\", \"x_2\", \"y_2\"]\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    ann_list = [os.path.join(ann_dir, i) for i in os.listdir(ann_dir)]\n",
    "    for ann_path in tqdm(ann_list):\n",
    "        with open(ann_path, 'r') as ann_file:\n",
    "            ann = json.load(ann_file)\n",
    "        # count number of items in annotation\n",
    "        noi = len([key for key in list(ann.keys()) if key.startswith(\"item\")])\n",
    "        # for each item in image form a row in df\n",
    "        for item_num in range(1, noi+1):\n",
    "            df_row = {key: None for key in columns}\n",
    "            df_row[\"id\"] = ann_path.split(\".\")[0].split(\"\\\\\")[-1]\n",
    "            df_row[\"image_path\"] = ann_path.replace(\"annos\", \"image\").split(\".\")[0] + \".jpg\"\n",
    "            df_row[\"pair_id\"] = ann[\"pair_id\"]\n",
    "            df_row[\"style\"] = ann[\"item\" + str(item_num)][\"style\"]\n",
    "            bbox = ann[\"item\" + str(item_num)][\"bounding_box\"]\n",
    "            df_row[\"x_1\"], df_row[\"y_1\"], df_row[\"x_2\"], df_row[\"y_2\"] = bbox\n",
    "            df_row = pd.DataFrame([df_row])\n",
    "            df = pd.concat([df, df_row], ignore_index=True)\n",
    "    # just checkpoint\n",
    "    df.to_csv('deepfashion.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32153/32153 [05:47<00:00, 92.64it/s] \n",
      "C:\\Users\\grk\\AppData\\Local\\Temp\\ipykernel_28068\\3120656711.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pairs['label'] = df_pairs.groupby(['pair_id', 'style']).ngroup()\n"
     ]
    }
   ],
   "source": [
    "# convert annos to pandas df\n",
    "convert_dataset(r\"data\\train\\annos\")\n",
    "# load checkpoint\n",
    "df = pd.read_csv(\"deepfashion.csv\", index_col=0)\n",
    "# we only take the items which possibly have pairs\n",
    "df_pairs = df[df['style'] != 0]\n",
    "# create label column\n",
    "df_pairs['label'] = df_pairs.groupby(['pair_id', 'style']).ngroup()\n",
    "# save df\n",
    "df_pairs.to_csv('deepfashion_train.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from pytorch_metric_learning import distances, losses, miners, reducers, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(7*7*64, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iteration 0: Loss = 0.10620295256376266, Number of mined triplets = 3129\n",
      "Epoch 1 Iteration 20: Loss = 0.11587725579738617, Number of mined triplets = 433\n",
      "Epoch 1 Iteration 40: Loss = 0.10214606672525406, Number of mined triplets = 699\n",
      "Epoch 1 Iteration 60: Loss = 0.09975206106901169, Number of mined triplets = 479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:57<00:00,  1.23it/s]\n",
      "100%|██████████| 71/71 [01:07<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.10866320377335961\n",
      "Epoch 2 Iteration 0: Loss = 0.10144372284412384, Number of mined triplets = 933\n",
      "Epoch 2 Iteration 20: Loss = 0.09729523956775665, Number of mined triplets = 681\n",
      "Epoch 2 Iteration 40: Loss = 0.10172709822654724, Number of mined triplets = 619\n",
      "Epoch 2 Iteration 60: Loss = 0.10664625465869904, Number of mined triplets = 346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:59<00:00,  1.19it/s]\n",
      "100%|██████████| 71/71 [01:11<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.12335064779986865\n",
      "Epoch 3 Iteration 0: Loss = 0.1058802530169487, Number of mined triplets = 898\n",
      "Epoch 3 Iteration 20: Loss = 0.10500578582286835, Number of mined triplets = 605\n",
      "Epoch 3 Iteration 40: Loss = 0.10518316179513931, Number of mined triplets = 712\n",
      "Epoch 3 Iteration 60: Loss = 0.09104671329259872, Number of mined triplets = 588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [01:25<00:00,  1.20s/it]\n",
      "100%|██████████| 71/71 [01:05<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.13188847095348977\n",
      "Epoch 4 Iteration 0: Loss = 0.09957858920097351, Number of mined triplets = 947\n",
      "Epoch 4 Iteration 20: Loss = 0.09831001609563828, Number of mined triplets = 671\n",
      "Epoch 4 Iteration 40: Loss = 0.09937715530395508, Number of mined triplets = 591\n",
      "Epoch 4 Iteration 60: Loss = 0.09839677810668945, Number of mined triplets = 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [01:02<00:00,  1.14it/s]\n",
      "100%|██████████| 71/71 [01:06<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.12048480506298884\n",
      "Epoch 5 Iteration 0: Loss = 0.1063404232263565, Number of mined triplets = 361\n",
      "Epoch 5 Iteration 20: Loss = 0.09988199174404144, Number of mined triplets = 874\n",
      "Epoch 5 Iteration 40: Loss = 0.10146575421094894, Number of mined triplets = 497\n",
      "Epoch 5 Iteration 60: Loss = 0.0985591784119606, Number of mined triplets = 930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [01:02<00:00,  1.14it/s]\n",
      "100%|██████████| 71/71 [01:03<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.12782852707624337\n",
      "Epoch 6 Iteration 0: Loss = 0.10968848317861557, Number of mined triplets = 541\n",
      "Epoch 6 Iteration 20: Loss = 0.09816888719797134, Number of mined triplets = 575\n",
      "Epoch 6 Iteration 40: Loss = 0.10574333369731903, Number of mined triplets = 480\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 87\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;66;03m### pytorch-metric-learning stuff ###\u001B[39;00m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, num_epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 87\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmining_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     88\u001B[0m     test(trainset, valset, model, accuracy_calculator)\n",
      "Cell \u001B[1;32mIn[15], line 4\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, loss_func, mining_func, device, train_loader, optimizer, epoch)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n\u001B[0;32m      3\u001B[0m     model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m----> 4\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (data, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m      5\u001B[0m         data, labels \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      6\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m    294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[1;32m--> 295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "Cell \u001B[1;32mIn[5], line 15\u001B[0m, in \u001B[0;36mDeepFashionDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     13\u001B[0m image \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage_path\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m     14\u001B[0m bbox \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx_1\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m], row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx_2\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m], row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_1\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m], row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_2\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m---> 15\u001B[0m cropped_image \u001B[38;5;241m=\u001B[39m \u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m label \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\PIL\\Image.py:1233\u001B[0m, in \u001B[0;36mImage.crop\u001B[1;34m(self, box)\u001B[0m\n\u001B[0;32m   1230\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCoordinate \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlower\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is less than \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mupper\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1231\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[1;32m-> 1233\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1234\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_crop(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mim, box))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\PIL\\ImageFile.py:269\u001B[0m, in \u001B[0;36mImageFile.load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    266\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(msg)\n\u001B[0;32m    268\u001B[0m b \u001B[38;5;241m=\u001B[39m b \u001B[38;5;241m+\u001B[39m s\n\u001B[1;32m--> 269\u001B[0m n, err_code \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    271\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "def train(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(data)\n",
    "        indices_tuple = mining_func(embeddings, labels)\n",
    "        loss = loss_func(embeddings, labels, indices_tuple)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(\n",
    "                \"Epoch {} Iteration {}: Loss = {}, Number of mined triplets = {}\".format(\n",
    "                    epoch, batch_idx, loss, mining_func.num_triplets\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "### convenient function from pytorch-metric-learning ###\n",
    "def get_all_embeddings(dataset, model):\n",
    "    tester = testers.BaseTester(dataloader_num_workers=0, batch_size=batch_size)\n",
    "    return tester.get_all_embeddings(dataset, model)\n",
    "\n",
    "\n",
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def test(train_set, test_set, model, accuracy_calculator):\n",
    "    train_embeddings, train_labels = get_all_embeddings(train_set, model)\n",
    "    test_embeddings, test_labels = get_all_embeddings(test_set, model)\n",
    "    train_labels = train_labels.squeeze(1)\n",
    "    test_labels = test_labels.squeeze(1)\n",
    "    print(\"Computing accuracy\")\n",
    "    accuracies = accuracy_calculator.get_accuracy(\n",
    "        test_embeddings, test_labels, train_embeddings, train_labels, False\n",
    "    )\n",
    "    print(\"Test set accuracy (Precision@1) = {}\".format(accuracies[\"precision_at_1\"]))\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "dataset = DeepFashionDataset(DATA_DIR, 'train', transform=train_transform)\n",
    "evens = list(range(0, len(dataset), 2))\n",
    "odds = list(range(1, len(dataset), 2))\n",
    "trainset = Subset(dataset, evens)\n",
    "valset = Subset(dataset, odds)\n",
    "#dataset2 = DeepFashionDataset(DATA_DIR, 'test', transform=val_transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "### pytorch-metric-learning stuff ###\n",
    "distance = distances.CosineSimilarity()\n",
    "reducer = reducers.ThresholdReducer(low=0)\n",
    "loss_func = losses.TripletMarginLoss(margin=0.2, distance=distance, reducer=reducer)\n",
    "mining_func = miners.TripletMarginMiner(\n",
    "    margin=0.2, distance=distance, type_of_triplets=\"semihard\"\n",
    ")\n",
    "accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), k=1)\n",
    "### pytorch-metric-learning stuff ###\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, loss_func, mining_func, device, train_loader, optimizer, epoch)\n",
    "    test(trainset, valset, model, accuracy_calculator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "# Load image as string from file/database    \n",
    "fd = open(r'1.raw',  encoding=\"latin1\")\n",
    "img_str = fd.read()\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fd = open('1.raw', 'r', encoding='latin1')\n",
    "rows = 480\n",
    "cols = 640\n",
    "f = np.fromfile(fd, dtype=np.uint8,count=rows*cols)\n",
    "im = f.reshape((rows, cols)) #notice row, column format\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "18125"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.utils"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
